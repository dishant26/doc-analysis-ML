{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip uninstall -y tensorflow\n!pip install transformers==2.8.0","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-11-28T19:41:56.423333Z","iopub.execute_input":"2022-11-28T19:41:56.423960Z","iopub.status.idle":"2022-11-28T19:42:31.327328Z","shell.execute_reply.started":"2022-11-28T19:41:56.423852Z","shell.execute_reply":"2022-11-28T19:42:31.326163Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Found existing installation: tensorflow 2.6.4\nUninstalling tensorflow-2.6.4:\n  Successfully uninstalled tensorflow-2.6.4\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting transformers==2.8.0\n  Downloading transformers-2.8.0-py3-none-any.whl (563 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m563.8/563.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers==2.8.0) (3.7.1)\nCollecting tokenizers==0.5.2\n  Downloading tokenizers-0.5.2-cp37-cp37m-manylinux1_x86_64.whl (5.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from transformers==2.8.0) (1.21.6)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from transformers==2.8.0) (0.1.97)\nRequirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers==2.8.0) (0.0.53)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers==2.8.0) (4.64.0)\nRequirement already satisfied: boto3 in /opt/conda/lib/python3.7/site-packages (from transformers==2.8.0) (1.24.93)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==2.8.0) (2021.11.10)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers==2.8.0) (2.28.1)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3->transformers==2.8.0) (1.0.1)\nRequirement already satisfied: botocore<1.28.0,>=1.27.93 in /opt/conda/lib/python3.7/site-packages (from boto3->transformers==2.8.0) (1.27.93)\nRequirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from boto3->transformers==2.8.0) (0.6.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==2.8.0) (3.3)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==2.8.0) (1.26.12)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==2.8.0) (2.1.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers==2.8.0) (2022.9.24)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==2.8.0) (8.0.4)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==2.8.0) (1.0.1)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from sacremoses->transformers==2.8.0) (1.15.0)\nRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.28.0,>=1.27.93->boto3->transformers==2.8.0) (2.8.2)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click->sacremoses->transformers==2.8.0) (4.13.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->sacremoses->transformers==2.8.0) (3.8.0)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->sacremoses->transformers==2.8.0) (4.1.1)\nInstalling collected packages: tokenizers, transformers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.12.1\n    Uninstalling tokenizers-0.12.1:\n      Successfully uninstalled tokenizers-0.12.1\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.20.1\n    Uninstalling transformers-4.20.1:\n      Successfully uninstalled transformers-4.20.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nallennlp 2.10.1 requires transformers<4.21,>=4.1, but you have transformers 2.8.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed tokenizers-0.5.2 transformers-2.8.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import os\n\n# Download and unzip movie substitle dataset\nif not os.path.exists('data/dataset.txt'):\n  !wget \"https://object.pouta.csc.fi/OPUS-OpenSubtitles/v2016/mono/es.txt.gz\" -O dataset.txt.gz\n  !gzip -d dataset.txt.gz\n  !mkdir data\n  !mv dataset.txt data","metadata":{"execution":{"iopub.status.busy":"2022-11-28T19:42:31.329539Z","iopub.execute_input":"2022-11-28T19:42:31.329943Z","iopub.status.idle":"2022-11-28T19:45:07.466941Z","shell.execute_reply.started":"2022-11-28T19:42:31.329902Z","shell.execute_reply":"2022-11-28T19:45:07.465634Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"--2022-11-28 19:42:32--  https://object.pouta.csc.fi/OPUS-OpenSubtitles/v2016/mono/es.txt.gz\nResolving object.pouta.csc.fi (object.pouta.csc.fi)... 86.50.254.19, 86.50.254.18\nConnecting to object.pouta.csc.fi (object.pouta.csc.fi)|86.50.254.19|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1859673728 (1.7G) [application/gzip]\nSaving to: ‘dataset.txt.gz’\n\ndataset.txt.gz      100%[===================>]   1.73G  17.3MB/s    in 1m 44s  \n\n2022-11-28 19:44:17 (17.0 MB/s) - ‘dataset.txt.gz’ saved [1859673728/1859673728]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import ViTFeatureExtractor, RobertaTokenizer, TrOCRProcessor, PreTrainedTokenizerFast\nfrom transformers import VisionEncoderDecoderModel\n\nencode = 'google/vit-base-patch16-224-in21k'\ndecode = 'GKLMIP/roberta-hindi-devanagari'\n\nfeature_extractor=ViTFeatureExtractor.from_pretrained(encode)\ntokenizer = RobertaTokenizer.from_pretrained(decode)\nprocessor = TrOCRProcessor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n\nfrom transformers import TrOCRProcessor\n\ntrain_dataset = IAMDataset(root_dir='../input/devanagiri-dataset/HindiSeg/',\n                           df=train_df,\n                           processor=processor)\neval_dataset = IAMDataset(root_dir='../input/devanagiri-dataset/HindiSeg/',\n                           df=test_df,\n                           processor=processor)\n\nmodel = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(encode, decode)\n\n# set special tokens used for creating the decoder_input_ids from the labels\nmodel.config.decoder_start_token_id = processor.tokenizer.cls_token_id\nmodel.config.pad_token_id = processor.tokenizer.pad_token_id\nprint(processor.tokenizer.pad_token_id)\n# make sure vocab size is set correctly\nmodel.config.vocab_size = model.config.decoder.vocab_size\nprint(model.config.vocab_size)\n# config_decoder.is_decoder = True\n# config_decoder.add_cross_attention = True\n\n# set beam search parameters\nmodel.config.eos_token_id = processor.tokenizer.sep_token_id\nmodel.config.max_length = 64\nmodel.config.early_stopping = True\nmodel.config.no_repeat_ngram_size = 3\nmodel.config.length_penalty = 2.0\nmodel.config.num_beams = 4","metadata":{"execution":{"iopub.status.busy":"2022-11-28T19:45:11.043242Z","iopub.execute_input":"2022-11-28T19:45:11.044320Z","iopub.status.idle":"2022-11-28T19:45:13.828509Z","shell.execute_reply.started":"2022-11-28T19:45:11.044275Z","shell.execute_reply":"2022-11-28T19:45:13.827096Z"},"trusted":true},"execution_count":3,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/966606905.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mViTFeatureExtractor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRobertaTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrOCRProcessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPreTrainedTokenizerFast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVisionEncoderDecoderModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mencode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'google/vit-base-patch16-224-in21k'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdecode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'GKLMIP/roberta-hindi-devanagari'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'ViTFeatureExtractor' from 'transformers' (/opt/conda/lib/python3.7/site-packages/transformers/__init__.py)"],"ename":"ImportError","evalue":"cannot import name 'ViTFeatureExtractor' from 'transformers' (/opt/conda/lib/python3.7/site-packages/transformers/__init__.py)","output_type":"error"}]},{"cell_type":"code","source":"count = 0\nwith open(\"/kaggle/input/devanagiri-dataset/hindi_vocab.txt\", \"r\") as f:\n    for line in f:\n        count +=1 \n        \ncount","metadata":{"execution":{"iopub.status.busy":"2022-11-23T07:28:36.554965Z","iopub.execute_input":"2022-11-23T07:28:36.555325Z","iopub.status.idle":"2022-11-23T07:28:36.576799Z","shell.execute_reply.started":"2022-11-23T07:28:36.555296Z","shell.execute_reply":"2022-11-23T07:28:36.575741Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"11030"},"metadata":{}}]},{"cell_type":"code","source":"# Total number of lines and some random lines\n!wc -l data/dataset.txt\n!shuf -n 5 data/dataset.txt","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get a subset of first 1,000,000 lines for training\nTRAIN_SIZE = 10000 #@param {type:\"integer\"}\n!(head -n $TRAIN_SIZE data/dataset.txt) > data/train.txt","metadata":{"execution":{"iopub.status.busy":"2022-11-23T07:25:33.459564Z","iopub.execute_input":"2022-11-23T07:25:33.460220Z","iopub.status.idle":"2022-11-23T07:25:34.563966Z","shell.execute_reply.started":"2022-11-23T07:25:33.460181Z","shell.execute_reply":"2022-11-23T07:25:34.562628Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Get a subset of next 10,000 lines for validation\nVAL_SIZE = 1030 #@param {type:\"integer\"}\n!(sed -n {TRAIN_SIZE + 1},{TRAIN_SIZE + VAL_SIZE}p data/dataset.txt) > data/dev.txt","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nfrom tokenizers import ByteLevelBPETokenizer\n\npath = \"data/train.txt\"\n\n# Initialize a tokenizer\ntokenizer = ByteLevelBPETokenizer()\n\n# Customize training\ntokenizer.train(files=path,\n                vocab_size=50265,\n                min_frequency=2,\n                special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"])\n\n# Save files to disk\n!mkdir -p \"models/roberta\"\ntokenizer.save(\"models/roberta\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nconfig = {\n\t\"architectures\": [\n\t\t\"RobertaForMaskedLM\"\n\t],\n\t\"attention_probs_dropout_prob\": 0.1,\n\t\"hidden_act\": \"gelu\",\n\t\"hidden_dropout_prob\": 0.1,\n\t\"hidden_size\": 768,\n    \"pad_token_id\": 1,\n\t\"initializer_range\": 0.02,\n\t\"intermediate_size\": 3072,\n\t\"layer_norm_eps\": 1e-05,\n\t\"max_position_embeddings\": 514,\n\t\"model_type\": \"roberta\",\n\t\"num_attention_heads\": 12,\n\t\"num_hidden_layers\": 12,\n\t\"type_vocab_size\": 1\n# \t\"vocab_size\": 50265\n}\n\nwith open(\"models/roberta/config.json\", 'w') as fp:\n    json.dump(config, fp)\n\ntokenizer_config = {\"max_len\": 512}\n\nwith open(\"models/roberta/tokenizer_config.json\", 'w') as fp:\n    json.dump(tokenizer_config, fp)","metadata":{},"execution_count":null,"outputs":[]}]}